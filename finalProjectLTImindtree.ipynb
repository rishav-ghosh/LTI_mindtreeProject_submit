{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bf9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb57d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/Users/rishavghosh/Desktop/python/Oasis_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1efa4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8337e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Mild Dementia', 'Moderate Dementia', 'Non Demented', 'Very mild Dementia']\n"
     ]
    }
   ],
   "source": [
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n",
    "print(\"Classes:\", full_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9ab7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 69149\n",
      "Test size: 17288\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7ee265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3988,   380, 53841, 10940])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_targets = [full_dataset.samples[i][1] for i in train_dataset.indices]\n",
    "train_targets\n",
    "class_counts = np.bincount(train_targets)\n",
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b8d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "print(\"Class weights:\", class_weights)\n",
    "sample_weights = [class_weights[label] for label in train_targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bc72bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class counts: {'Mild Dementia': 4028, 'Moderate Dementia': 389, 'Non Demented': 53759, 'Very mild Dementia': 10973}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Extract labels from train dataset\n",
    "train_targets = [full_dataset.samples[i][1] for i in train_dataset.indices]\n",
    "class_counts = np.bincount(train_targets)\n",
    "\n",
    "print(\"Train class counts:\", dict(zip(full_dataset.classes, class_counts)))\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "sample_weights = [class_weights[label] for label in train_targets]\n",
    "\n",
    "# Sampler for balanced training\n",
    "train_sampler = WeightedRandomSampler(weights=sample_weights,\n",
    "                                      num_samples=len(sample_weights),\n",
    "                                      replacement=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27f89247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eefbee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled counts: Counter({1: 17301, 3: 17299, 0: 17277, 2: 17272})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "labels_list = []\n",
    "for _, labels in train_loader:\n",
    "    labels_list.extend(labels.tolist())\n",
    "\n",
    "print(\"Sampled counts:\", Counter(labels_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f94ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BrainCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 28 * 28, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617360ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rishavghosh/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading helper_functions.py\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path \n",
    "\n",
    "# Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "  print(\"helper_functions.py already exists, skipping download\")\n",
    "else:\n",
    "  print(\"Downloading helper_functions.py\")\n",
    "  # Note: you need the \"raw\" GitHub URL for this to work\n",
    "  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "  with open(\"helper_functions.py\", \"wb\") as f:\n",
    "    f.write(request.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7e4735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from helper_functions import accuracy_fn\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model_1 = BrainCNN(num_classes=len(full_dataset.classes)).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_1.parameters(), lr=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b2adfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "def print_train_time(start: float, end: float, device: torch.device = None):\n",
    "    \"\"\"Prints the training time.\n",
    "\n",
    "    Args:\n",
    "        start (float): Time when training started.\n",
    "        end (float): Time when training ended.\n",
    "        device (torch.device, optional): Device used for training. Defaults to None.\n",
    "    \"\"\"\n",
    "    total_time = end - start\n",
    "    print(f\"Training time on {device}: {total_time:.3f} seconds\")\n",
    "    return total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aeaa199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, loss_fn: torch.nn.Module, optimizer: torch.optim.Optimizer, device: torch.device = device, accuracy_fn = accuracy_fn):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.to(device)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "        train_acc += accuracy_fn(y_true = y, y_pred = y_pred.argmax(dim=1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss /= len(data_loader)\n",
    "    train_acc /= len(data_loader)\n",
    "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader, model: torch.nn.Module, loss_fn: torch.nn.Module, device: torch.device = device, accuracy_fn = accuracy_fn):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "    with torch.inference_mode(): \n",
    "        for X, y in data_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "        \n",
    "            test_pred = model(X)\n",
    "        \n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss.item()   # <--- FIXED\n",
    "        \n",
    "            test_acc += accuracy_fn(\n",
    "                y_true=y,\n",
    "                y_pred=test_pred.argmax(dim=1)\n",
    "            )\n",
    "\n",
    "    test_loss /= len(data_loader)\n",
    "    test_acc /= len(data_loader)\n",
    "    print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0f496cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "---------\n",
      "Train loss: 0.34780 | Train accuracy: 83.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [10:28<20:57, 628.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.43020 | Test accuracy: 80.57%\n",
      "\n",
      "Epoch: 1\n",
      "---------\n",
      "Train loss: 0.26644 | Train accuracy: 87.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [21:21<10:42, 642.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.36189 | Test accuracy: 81.91%\n",
      "\n",
      "Epoch: 2\n",
      "---------\n",
      "Train loss: 0.21339 | Train accuracy: 89.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [32:07<00:00, 642.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.37055 | Test accuracy: 84.20%\n",
      "\n",
      "Training time on mps: 1927.786 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Measure time\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "train_time_start_on_gpu = timer()\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print(f\"Epoch: {epoch}\\n---------\")\n",
    "    train_step(data_loader=train_loader, \n",
    "        model=model_1, \n",
    "        loss_fn=loss_fn,\n",
    "        optimizer=optimizer,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "    test_step(data_loader=test_loader,\n",
    "        model=model_1,\n",
    "        loss_fn=loss_fn,\n",
    "        accuracy_fn=accuracy_fn\n",
    "    )\n",
    "\n",
    "train_time_end_on_gpu = timer()\n",
    "total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n",
    "                                            end=train_time_end_on_gpu,\n",
    "                                            device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd04b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_1.state_dict(), \"brainScan_cnnLTImindtree.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184a7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
